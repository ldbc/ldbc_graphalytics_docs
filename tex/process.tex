\chapter{Benchmark Process}
\label{chap:benchmark_process}
The Graphalytics benchmark suite is developed to facilitate the benchmark process described in this technical specification. This chapter describes the benchmark composition, the benchmark type, the detailed steps of the benchmark execution, and the format of the benchmark report. 


\section{Benchmark}
\label{sec:process:benchmark}
A benchmark is a standardized process to quantify the performance of the system under test. \autoref{fig:benchmark_composition} depicts the benchmark composition: each benchmark contains a set of benchmark experiments, each experiment consists of multiple benchmark jobs, and each job is executed repeatedly in the form of benchmark runs.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.6\linewidth]{figures/benchmark-composition.pdf}
	\caption{The composition of a benchmark.}
	\label{fig:benchmark_composition}
\end{figure}

\begin{itemize}
    \item A \textbf{benchmark experiment} addresses a specific performance characteristic of the system under test, e.g., the performance of an algorithm, or the weak scalability of a system. Each experiment gathers benchmark results from multiple benchmark jobs to quantify a specific performance characteristic.
    
    \item A \textbf{benchmark job} describes, uniformly across all system under tests, the exact job specification of a graph-processing job (see \autoref{sec:def:job}). The job specification contains information, e.g., the system under test (the platform and the environment), the type of algorithm and dataset, and how much resources are used. These information instructs how the system should be configured during the benchmark execution.
    
    \item A \textbf{benchmark run} is a real-world execution of a benchmark job. To gather statistically reliable benchmark results, each benchmark job is repeated multiple times in the form of a benchmark run to mitigate the performance variability during the benchmark execution. 
\end{itemize}




\section{Benchmark Type}
\label{sec:process:type}
The Graphalytics benchmark suite supports four types of benchmark: $\textit{test}$, $\textit{standard}$, $\textit{full}$, and $\textit{custom}$. This section describes the differences and the composition of these four benchmark types.


% \subsection{Test Benchmark}
% \label{sec:process:type:test}
% Running the entire benchmark is very time-consuming. To verify that the cluster environment and all underlying software and hardware stacks are configured properly, the user can choose to first run the test benchmark before starting the actual benchmark. In addition, this can be used to debug a platform driver which is under development.

% The test benchmark consists of two experiments which run all six core algorithms on two tiny graphs: ``example-directed'' and ``example-undirected'', with only one repetition. In total, there are $6 \times 2 \times 1 =  12$ benchmark runs.


% \begin{table}[H]
% \centering
% \begin{tabular*}{0.8\textwidth}{| l@{\extracolsep{\fill}} l |}
% \hline
% {\bf Experiments}  & \\ \hline
% directed ($d$), undirected ($u$) & \\ \hline
% \end{tabular*}
% \quad 
% \begin{tabular*}{0.8\textwidth}{| c@{\extracolsep{\fill}} | c | c | c | c | c | c |}
% \hline
% {\bf Algorithm} & {\bf Dataset} & {\bf Job} & {\bf Repetition} & {\bf Runs} & {\bf Scale} & {\bf Time-out}    \\ \hline
% 6 & 2  & 12 & 1 & 12 & $ < 0.5$ & 10 min \\ \hline
% \end{tabular*}
% \quad 
% \begin{tabular*}{0.8\textwidth}{| c@{\extracolsep{\fill}} | c | c | c | c | c | c |}
% \hline
% {\bf Job} & {\bf BFS} & {\bf WCC} & {\bf PR} & {\bf CDLP} & {\bf LCC} & {\bf SSSP}  \\ 
% \hline
% {\bf XDIR} & $d$ & $d$ & $d$ & $d$ & $d$ & $d$ \\ \hline
% {\bf XUDIR} & $u$ & $u$ & $u$ & $u$ & $u$ & $u$ \\ \hline 
% \end{tabular*}
% \caption{The benchmark composition of the Test benchmark.}
% \label{tab:test_benchmark}
% \end{table}


\subsection{Competition Benchmark}
\label{sec:process:type:standard}
Participating in the Graphalytics competition executes the benchmark for ecah data set size category.

The competition benchmark evaluates the system performance with six core algorithms, BFS, WCC, PR, CDLP, LCC, SSSP (see \autoref{sec:definition_algorithms}). For each algorithm, the datasets within a given size category are used. A competition benchmark can fall into one of the five target scales: S, M, L, XL, and 2XL+. Each target scale focuses on processing graphs within certain range of data size, and therefore a corresponding time-out duration has been imposed as shown in \autoref{tab:competition-benchmarks}.

\begin{table}[htbp]
	\centering
	\begin{tabular}{|l|r|}
		\hline
		\multicolumn{1}{|c|}{\bf Size} & \multicolumn{1}{c|}{\bf Timeout} \\
		\hline
		S          & 15 minutes    \\
		M          & 30 minutes    \\
		L          & 1 hour        \\
		XL         & 2 hours       \\
		2XL+       & 3 hours       \\
		\hline
	\end{tabular}
	\caption{Benchmarks used in the competition}
	\label{tab:competition-benchmarks}
\end{table}

Each algorithm is executed 3 times.

\section{Benchmark Execution}
\label{sec:process:execution}
The benchmark execution of Graphalytics benchmark suite is illustrated in \autoref{fig:benchmark-process}. This section explains how the benchmark suite executes a benchmark with regard to its execution flow, run flow, data flow, metric collection, and failure indication.

\begin{figure}[h]
 	\centering
 	\includegraphics[width=0.9\linewidth]{figures/benchmark_process.pdf}
 	\caption{Benchmark execution in the Graphalytics benchmark suite.}
 	\label{fig:benchmark-process}
\end{figure}


\subsection{Execution Flow}
\label{sec:process:execution:exe_flow}
After a benchmark is loaded, the benchmark suite analyzes the exact composition of the benchmark. Each benchmark consists of a number of benchmark runs, which will be grouped by the graph dataset used by that benchmark run. For each graph dataset, the input data of that dataset will be loaded only once, and be reused for all corresponding benchmarks runs, before finally being removed.

\begin{enumerate}[label=\textbf{[\Alph*]}]
    \item \textbf{Verify-setup:} The benchmark suite verifies that the platform and the environment are properly set up based on the prerequisites defined in the platform driver.
    
    \item \textbf{Format-graph:} The benchmark suite minimizes the ``input data'' into ``formatted dataset'' (see more in \autoref{sec:process:execution:run_flow}) by removing unused vertex and edge properties.
    
    \item \textbf{Load-graph:} The platform converts the ``formatted data'' into any platform-specific data format and loads a graph dataset into a storage system, which can be either a local file system, a share file system or a distributed file system. This step corresponds to the ``Loading'' step of a graph processing job described in \autoref{sec:def:job:operation}.
    
    \item \textbf{Execute-run:} The platform executes a benchmark run with a specific algorithm and dataset (see more details in \autoref{sec:process:execution:run_flow}). All benchmark runs using the same input dataset can use the prepared graph dataset during the ``load-graph'' step. 
    
    \item \textbf{Delete-graph:} The platform unloads a graph dataset from the storage system, as part of the cleaning up process after all benchmark runs on that graph dataset have been completed.
\end{enumerate}

Note that ``load-graph'' and ``delete-graph'' are platform-specific API, which can be implemented in the platform driver via the ``Platform'' interface, whereas  ``execute-run'' is a uniform step for all platforms. \futureinversion{2.0}{What about format-graph? [Gabor]}

\subsection{Run Flow}
\label{sec:process:execution:run_flow}
The execution of each benchmark run consists of seven steps in total, i.e., ``prepare'', ``startup'', ``run'', ``validate'', ``finalize'', ``terminate'', and  ``archive''. To ensure the stability, the benchmark suite only prepares for the benchmark, and terminates the benchmark run. Each benchmark run is partially executed in an isolated operating-system process, such that a timed-out job can be terminated properly.

\begin{enumerate}[label=\textbf{[\arabic*]}]
    \item \textbf{Prepare:} The platform requests computation resources from the cluster environment and makes the background applications ready.
    
    \item \textbf{Startup:} The platform configures the benchmark run with regard to real-time cluster deployment information, e.g., input directory, output directory and log directory.
    
    \item \textbf{Run:} The platform runs a graph-processing job as defined in the benchmark run. The graph-processing job must complete within the time-out duration, or the benchmark run will fail. This step corresponds to the ``Running'' step of a graph processing job described in \autoref{sec:def:job:operation}.
    
    \item \textbf{Validate:} The benchmark suite validates the platform output with the validation data. The system under test must succeed in this step, or the benchmark run will fail.
    
    \item \textbf{Finalize:} The platform reports the benchmark information and makes the environment ready for the next benchmark run.
    
    \item \textbf{Terminate:} The platform forcibly stops the benchmark job and cleans up the environment, given that the time-out has been reached. 
    
    \item \textbf{Archive:} The benchmark suite archives the benchmark results, gathering information regarding performance metrics and failure indications.
\end{enumerate}

Note that ``prepare'', ``startup'', ``run'', ``finalize'', ``terminate'', and ``archive'' are platform-specific API, which can be implemented in the platform driver via the ``Platform'' interface, whereas ``archive'' is a uniform step for all platforms. \futureinversion{2.0}{The ``archive'' step belongs to the Core API. What about ``validate''? [Gabor]} 


\subsection{Data Flow}
\label{sec:process:execution:data_flow}
The graph datasets go through a series of execution steps during the benchmark execution, and in the process of which the format, the representation, and the content of the graph dataset change accordingly.

For each graph, the input datasets and the validation datasets are publicly available benchmark resources.

\begin{itemize} 
    \item \textbf{Input data:} The ``input data'' consists of a vertex file and edge file in EVLP format, as defined in \autoref{sec:data:representation}.
    \item \textbf{Validation data:} The ``validation data'' consists of correct outputs for all six core algorithm, as defined in \autoref{sec:definitions_validation}.
\end{itemize}

The input dataset can be converted into the following format during the benchmark.

\begin{itemize}
    \item \textbf{Formatted data:} The ``input data'' can plausibly contain dozens of vertex and edge properties. During the ``load-graph'' step, the benchmark suite identifies for each algorithms which properties are needed and which are not, and minimizes the ``input data'' into the ``formatted data''. The ``formatted data'' is cached in the storage system for future uses. 
    \item \textbf{Loaded data:} The minimized ``formatted data'' is loaded into a storage system during the ``load-graph'' step, which can either be a local file system, a share file system or a distributed file system.
    \item \textbf{Output data:} The ``output data'' is the output of a graph-processing job being benchmarked during the ``process'' step. The ``output data'' is compared to the ``validation data'' to ensure the correctness of the benchmark execution.
\end{itemize}



\subsection{Failure Indication}
\label{sec:process:execution:failure}
Failures can occur during the benchmark for many reasons. The benchmark suite logs the benchmark execution and classifies the type of failures.


\begin{itemize}
    \item \texttt{DAT}: ``Data failure'' occurs when the ``format-graph`` step fails to generate ``formatted-graph``, or the ``load-graph'' step fails to complete correctly. For example, ``input graph'' can be missing or simply be misplaced, or alternatively the conversion from ``input-graph'' to ``formatted-graph'' could be prematurely interrupted, which leads to data corruption.
    
    \item \texttt{INI}: ``Initialization failure'' occurs when the platform fails to properly make the environment ready for the benchmark during the ``prepare'' or ``startup'' step. For example, the deployment system may fail to allocate the cluster resources needed.
    
    \item \texttt{EXE}: ``Execution failure'' occurs when the execution of the benchmark run fails to complete during the ``run'' step.
    
    \item \texttt{TIM}: ``Time-out failure'' occurs when the pre-defined time-out duration is reached during the ``run'' step.

    \item \texttt{COM}: ``Completion failure'' occurs when output results are incomplete or cannot be found at all. For example, outputs from some compute nodes can be fetched incorrectly.
    
    \item \texttt{VAL}: ``Validation failure'' occurs when the ``output data'' is returned by the system, but fails the validation during the ``validate'' step.
    
    \item \texttt{MET}: ``Metric failure'' occurs when the compulsory performance metrics are missing during the ``archive'' step. For example, the log files containing the information can be non-existing or corrupted.
\end{itemize}










\section{Benchmark Result}

A complete result for the Graphalytics benchmark includes at least the following information: 

\begin{enumerate}
	\item Target scale (T).
	\item Environment specification, including number and type of CPUs, amount of memory, type of network, etc.
	\item Versions of the platform and Graphalytics drivers used in the experiments.
	\item Any non-default configuration options for the platform required to reproduce the system under test.
	\item For every benchmark job:
		\begin{enumerate}
			\item Job specification, i.e., dataset and algorithm.
			\item For every platform run, report the measured processing time, makespan, and whether the run breached the Graphalytics SLA.
			\item (optional) {\tt Granula} archives for each platform run, enabling deep inspection, visualization, modeling, and sharing of performance data.  
		\end{enumerate}
		
\futureinversion{2.0}{
	\item If scalability experiments are performed:
		\begin{enumerate}
			\item Definition of the hardware resources that were scaled up for each scalability experiment.
			\item For every benchmark job corresponding to a scalability experiment, include the resource scale (i.e., 1, 2, 4, 8, or 16).
		\end{enumerate}
}

\futureinversion{2.0}{
	\item 
	If robustness experiments are performed:
		\begin{enumerate}
			\item Summary of the results for every benchmark job.
		\end{enumerate}	
}

\end{enumerate}
Future versions of the benchmark specification will include a Full Disclosure Report template and a process for submitting official Graphalytics results. A sample data format can be found in Appendix~\ref{chap:data-format}.

